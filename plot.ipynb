{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           avg_acc1  avg_acc5\n",
      "mobilemclip_s1_6m_0_1      0.186394       NaN\n",
      "mobilemclip_s1_6m_025_075  0.176658       NaN\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line_number, line in enumerate(f, 1):\n",
    "            try:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    json_obj = json.loads(line)\n",
    "                    data.append(json_obj)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing line {line_number} in {file_path}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error on line {line_number} in {file_path}: {e}\")\n",
    "    return data\n",
    "\n",
    "def process_data(data, model_name):\n",
    "    processed = {}\n",
    "    for item in data:\n",
    "        dataset = item['dataset'].split('/')[-1]\n",
    "        metrics = item['metrics']\n",
    "        if 'acc1' in metrics:\n",
    "            processed[f\"{dataset}_acc1\"] = metrics['acc1']\n",
    "        if 'acc5' in metrics:\n",
    "            processed[f\"{dataset}_acc5\"] = metrics['acc5']\n",
    "    return {model_name: processed}\n",
    "\n",
    "def calculate_average_scores(all_data, models):\n",
    "    \"\"\"\n",
    "    Calculate the average acc1 and acc5 scores for each model.\n",
    "    \"\"\"\n",
    "    average_scores = {}\n",
    "    \n",
    "    for model in models:\n",
    "        acc1_scores = []\n",
    "        acc5_scores = []\n",
    "        \n",
    "        if model in all_data:\n",
    "            for metric, score in all_data[model].items():\n",
    "                if \"acc1\" in metric:\n",
    "                    acc1_scores.append(score)\n",
    "                if \"acc5\" in metric:\n",
    "                    acc5_scores.append(score)\n",
    "                    \n",
    "        # Calculate averages if scores are available\n",
    "        avg_acc1 = np.mean(acc1_scores) if acc1_scores else None\n",
    "        avg_acc5 = np.mean(acc5_scores) if acc5_scores else None\n",
    "        \n",
    "        average_scores[model] = {\n",
    "            'avg_acc1': avg_acc1,\n",
    "            'avg_acc5': avg_acc5\n",
    "        }\n",
    "    \n",
    "    return average_scores\n",
    "\n",
    "# Load and process the data\n",
    "models = ['mobilemclip_s1_6m_0_1', 'mobilemclip_s1_6m_025_075']\n",
    "all_data = {}\n",
    "\n",
    "for model in models:\n",
    "    file_path = f'results/{model}.jsonl'\n",
    "    try:\n",
    "        data = load_data(file_path)\n",
    "        all_data.update(process_data(data, model))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Calculate average scores and display them\n",
    "average_scores = calculate_average_scores(all_data, models)\n",
    "average_scores_df = pd.DataFrame(average_scores).T\n",
    "\n",
    "print(average_scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           avg_acc1  avg_acc5\n",
      "mobilemclip_s1_6m_0_1      0.186394  0.424934\n",
      "mobilemclip_s1_6m_025_075  0.176658  0.431909\n"
     ]
    }
   ],
   "source": [
    "def process_data_with_nan(data, model_name):\n",
    "    \"\"\"\n",
    "    Process the data, handling NaN values by treating them as None.\n",
    "    \"\"\"\n",
    "    processed = {}\n",
    "    for item in data:\n",
    "        dataset = item['dataset'].split('/')[-1]\n",
    "        metrics = item['metrics']\n",
    "\n",
    "        acc1 = metrics.get('acc1', None)\n",
    "        acc5 = metrics.get('acc5', None)\n",
    "\n",
    "        # Handle NaN values explicitly, setting them to None\n",
    "        if isinstance(acc5, float) and np.isnan(acc5):\n",
    "            acc5 = None\n",
    "\n",
    "        if acc1 is not None:\n",
    "            processed[f\"{dataset}_acc1\"] = acc1\n",
    "        if acc5 is not None:\n",
    "            processed[f\"{dataset}_acc5\"] = acc5\n",
    "\n",
    "    return {model_name: processed}\n",
    "\n",
    "# Modify the main loading and processing logic to use the new function\n",
    "all_data = {}\n",
    "\n",
    "for model in models:\n",
    "    file_path = f'results/{model}.jsonl'\n",
    "    try:\n",
    "        data = load_data(file_path)\n",
    "        all_data.update(process_data_with_nan(data, model))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Recalculate average scores and display them\n",
    "average_scores = calculate_average_scores(all_data, models)\n",
    "average_scores_df = pd.DataFrame(average_scores).T\n",
    "\n",
    "print(average_scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Model   IN-val  IN-shift  \\\n",
      "mobilemclip_s1_6m_1_0          mobilemclip_s1_6m_1_0  0.18320  0.239230   \n",
      "mobilemclip_s1_6m_025_075  mobilemclip_s1_6m_025_075  0.20142  0.252474   \n",
      "\n",
      "                           Flickr30k T→I  Flickr30k I→T  COCO T→I  COCO I→T  \\\n",
      "mobilemclip_s1_6m_1_0             0.2096          0.279  0.139384    0.1880   \n",
      "mobilemclip_s1_6m_025_075         0.2272          0.260  0.151819    0.2044   \n",
      "\n",
      "                           Avg. Perf. on 23  \n",
      "mobilemclip_s1_6m_1_0              0.508060  \n",
      "mobilemclip_s1_6m_025_075          0.523343  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line_number, line in enumerate(f, 1):\n",
    "            try:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    json_obj = json.loads(line)\n",
    "                    data.append(json_obj)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing line {line_number} in {file_path}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error on line {line_number} in {file_path}: {e}\")\n",
    "    return data\n",
    "\n",
    "def process_data(data, model_name, selected_datasets):\n",
    "    processed = {\n",
    "        'zeroshot_cls': {},\n",
    "        'flickr30k_ret': {},\n",
    "        'coco_ret': {},\n",
    "        'avg_perf': 0\n",
    "    }\n",
    "    dataset_scores = []\n",
    "\n",
    "    for item in data:\n",
    "        dataset = item['dataset'].split('/')[-1]\n",
    "        metrics = item['metrics']\n",
    "\n",
    "        if dataset == 'imagenet1k':\n",
    "            processed['zeroshot_cls']['IN-val'] = metrics.get('acc5', 0)\n",
    "        elif dataset in ['imagenetv2', 'imagenet-r', 'imagenet-o', 'imagenet_sketch']:\n",
    "            processed['zeroshot_cls'][dataset] = metrics.get('acc5', 0)\n",
    "        elif dataset == \"flickr30k\":\n",
    "            processed['flickr30k_ret']['I2T'] = metrics.get('image_retrieval_recall@5', 0)\n",
    "            processed['flickr30k_ret']['T2I'] = metrics.get('text_retrieval_recall@5', 0)\n",
    "        elif dataset == \"mscoco_captions\":\n",
    "            processed['coco_ret']['I2T'] = metrics.get('image_retrieval_recall@5', 0)\n",
    "            processed['coco_ret']['T2I'] = metrics.get('text_retrieval_recall@5', 0)\n",
    "\n",
    "        if dataset in selected_datasets:\n",
    "            if 'acc5' in metrics:\n",
    "                dataset_scores.append(metrics['acc5'])\n",
    "            # recall@1은 flickr30k, coco_captions 에서만 사용되므로, 다른 데이터셋은 제외\n",
    "            elif dataset in [\"flickr30k\", \"coco_captions\"] and any(k.startswith('recall@1') for k in metrics):\n",
    "                # recall@1_image2text 또는 recall@1_text2image 중 하나라도 있으면 사용\n",
    "                dataset_scores.append(metrics.get('image_retrieval_recall@5', metrics.get('text_retrieval_recall@5', 0)))\n",
    "\n",
    "\n",
    "    if dataset_scores:\n",
    "        processed['avg_perf'] = np.mean(dataset_scores)\n",
    "\n",
    "    return {model_name: processed}\n",
    "\n",
    "\n",
    "def visualize_retrieval(all_data, models):\n",
    "    df_data = {}\n",
    "    for model_name, model_data in all_data.items():\n",
    "        row = {\n",
    "            'Model': model_name,\n",
    "            'IN-val': model_data['zeroshot_cls'].get('IN-val', 0),\n",
    "            'IN-shift': np.mean([v for k, v in model_data['zeroshot_cls'].items() if k != 'IN-val']),\n",
    "            'Flickr30k T→I': model_data['flickr30k_ret'].get('I2T', 0),\n",
    "            'Flickr30k I→T': model_data['flickr30k_ret'].get('T2I', 0),\n",
    "            'COCO T→I': model_data['coco_ret'].get('I2T', 0),\n",
    "            'COCO I→T': model_data['coco_ret'].get('T2I', 0),\n",
    "            'Avg. Perf. on 23': model_data['avg_perf']\n",
    "        }\n",
    "        df_data[model_name] = row\n",
    "    df = pd.DataFrame.from_dict(df_data, orient='index')\n",
    "    print(df)\n",
    "    return df\n",
    "\n",
    "# --- 메인 실행 부분 ---\n",
    "models = ['mobilemclip_s1_6m_1_0', 'mobilemclip_s1_6m_025_075', 'mobilemclip_s1_6m_075_025', 'mobilemclip_s1_6m_0_1']\n",
    "all_data = {}\n",
    "\n",
    "selected_datasets = [\n",
    "    \"objectnet\", \"fer2013\", \"voc2007\", \"sun397\", \"cars\", \"mnist\", \"stl10\", \"gtsrb\",\n",
    "    \"cifar10\", \"cifar100\", \"imagenet1k\", \"pets\", \"clevr_closest_object_distance\",\n",
    "    \"caltech101\", \"svhn\", \"dmlab\", \"eurosat\", \"diabetic_retinopathy\", \"resisc45\",\n",
    "    \"imagenetv2\", \"imagenet_sketch\", \"imagenet-r\", \"imagenet-o\"\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    file_path = f'results/{model}.jsonl'\n",
    "    try:\n",
    "        data = load_data(file_path)\n",
    "        all_data.update(process_data(data, model, selected_datasets))\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "df = visualize_retrieval(all_data, models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
